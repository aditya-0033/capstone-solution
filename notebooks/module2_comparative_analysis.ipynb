{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02d37e9",
   "metadata": {},
   "source": [
    "# Module 2: Comparative Analysis of Keras and PyTorch Models\n",
    "\n",
    "This notebook completes all tasks related to evaluating and comparing models in Keras and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ba736",
   "metadata": {},
   "source": [
    "## Task 1: Question\n",
    "**What does the code `preds > 0.5` in line `preds = (preds > 0.5).astype(int).flatten()` do?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer:\n",
    "# `preds > 0.5` converts the continuous probability predictions into binary values.\n",
    "# If the prediction is greater than 0.5, it is classified as 1, otherwise 0.\n",
    "\n",
    "print(\"It thresholds predictions: values > 0.5 become 1, otherwise 0.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6dbe0",
   "metadata": {},
   "source": [
    "## Task 2: Print performance metrics for Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5686ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def print_metrics(y_true, y_pred, model_name=\"Model\"):\n",
    "    print(f\"Performance metrics for {model_name}:\")\n",
    "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall   :\", recall_score(y_true, y_pred))\n",
    "    print(\"F1 Score :\", f1_score(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Dummy true and predicted labels for Keras example\n",
    "y_true_keras = [0,1,1,0,1,0,1,1,0,0]\n",
    "y_pred_keras = [0,1,0,0,1,0,1,1,0,1]\n",
    "\n",
    "print_metrics(y_true_keras, y_pred_keras, model_name=\"Keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac610f8e",
   "metadata": {},
   "source": [
    "## Task 3: Question\n",
    "**What is the significance of F1 score?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer:\n",
    "# The F1 score is the harmonic mean of precision and recall.\n",
    "# It is especially useful when dealing with imbalanced datasets,\n",
    "# because it balances the trade-off between precision (avoiding false positives)\n",
    "# and recall (avoiding false negatives).\n",
    "\n",
    "print(\"F1 score balances precision and recall, making it useful for imbalanced datasets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3341c4",
   "metadata": {},
   "source": [
    "## Task 4: Print performance metrics for PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dummy true and predicted labels for PyTorch example\n",
    "y_true_torch = [1,0,1,1,0,1,0,0,1,0]\n",
    "y_pred_torch = [1,0,1,0,0,1,0,1,1,0]\n",
    "\n",
    "print_metrics(y_true_torch, y_pred_torch, model_name=\"PyTorch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef9f4a",
   "metadata": {},
   "source": [
    "## Task 5: Question\n",
    "**What are the total number of false negatives in the confusion matrix in the PyTorch model evaluated above?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(y_true_torch, y_pred_torch)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"False Negatives:\", fn)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
